# robots.txt 介绍

## 概况

robots协议也叫robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（spider），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为**小写**。robots.txt应放置于网站的**根目录**下。如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据。

示例：

> * ```https://www.taobao.com/robots.txt```
> * ```https://www.jd.com/robots.txt```

## 行为

robots.txt 文件规定了搜索引擎抓取工具可以/无法请求抓取您网站上的哪些网页或文件。
此文件主要用于网站避免收到过多请求；它并不是一种用于阻止网络搜索引擎访问某个网页的机制。

## 文件位置和有效范围

robots.txt 文件必须位于主机的顶级目录中，可通过适当的协议和端口号进行访问。robots.txt 的通用协议都是基于 URI 的协议。按照 http 和 https 协议，使用 HTTP 无条件 GET 请求来抓取 robots.txt 文件。

robots.txt 文件中列出的指令仅适用于该文件所在的主机、协议和端口号。
> 即 ip:port/robots.txt 的限定仅适用于 ip:port/ 可以访问的位置，对于 ip/ 可以访问到的位置，此robots.txt不会影响

## 文件格式

预期的文件格式是 UTF-8 编码的纯文本。文件包含由 CR、CR/LF 或 LF 分隔的多个行。

系统将只考虑有效的行，而忽略其他所有内容。例如，如果获得的文档为 HTML 网页，则系统只会考虑网页中有效的文本行，而忽略其他内容，并且既不显示警告也不报告错误。

如果因为使用某种字符编码而引入了不属于 UTF-8 子集的字符，则可能导致文件内容解析错误。

系统会忽略 robots.txt 文件开头可选的 Unicode BOM（字节顺序标记）。

## 用户代理的优先顺序

对于一个抓取工具而言，只有一个组是有效的。抓取工具必须查找最具体的匹配用户代理，从而确定正确的行组。抓取工具会忽略其他所有组。用户代理区分大小写。所有非匹配文本都会被忽略（例如，googlebot/1.2 和 googlebot* 均等同于 googlebot）。这与 robots.txt 文件中的组顺序无关。

如果特定用户代理有多个组，则这些组中适用于特定用户代理的所有规则会合并在一起。

## 组成员规则

本部分仅说明标准的组成员规则。对于抓取工具，这些规则也称为“指令”。这些指令以 directive: [path] 的形式指定，其中 [path] 可选。默认情况下，指定的抓取工具没有抓取限制。没有 [path] 的指令将被忽略。

如果指定了 [path] 值，该路径值将被视作 robots.txt 文件抓取网站的根目录的相对路径（使用相同的协议、端口号、主机和域名）。路径值必须以“/”开头，表示根目录。

**disallow**

disallow 指令指定相应抓取工具不能访问的路径。如果未指定路径，该指令将被忽略。

用法：

> disallow: [path]

**allow**

allow 指令指定相应抓取工具可以访问的路径。如果未指定路径，该指令将被忽略。

用法：

> allow: [path]

## 基于路径值的网址匹配

以路径值为基础确定某项规则是否适用于网站上的特定网址。不使用通配符时，路径可用于匹配网址的开头（以及以相同路径开头的任何有效网址）。路径中的非 7 位 ASCII 字符可以作为 UTF-8 字符添加，也可以按照 RFC 3986 作为百分号转义的 UTF-8 编码字符添加。

对于路径值，大部分爬虫支持有限形式的“通配符”。这些通配符包括：

> * \* 表示任何有效字符的 0 个或多个个案。
> * $ 表示网址结束。

## 组成员行的优先顺序

在组成员一级，尤其是对于 allow 和 disallow 指令，最具体的规则（根据 [path] 条目的长度，长度越短，越不具体）优先级最高。如果规则（包括使用通配符的规则）存在冲突，系统将使用限制性最弱的规则。
